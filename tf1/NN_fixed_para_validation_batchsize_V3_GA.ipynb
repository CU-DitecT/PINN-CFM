{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt; plt.rcParams['font.size'] = 16\n",
    "import sys\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "if 1:\n",
    "    np.random.seed(1234)\n",
    "    tf.set_random_seed(1234)\n",
    "    RANDOM_SEED = 1234\n",
    "else:\n",
    "    np.random.seed(4321)\n",
    "    tf.set_random_seed(4321)\n",
    "    RANDOM_SEED = 4321\n",
    "    \n",
    "def xvfl_to_feature(arr):\n",
    "    #dx\n",
    "    dx = arr[:,2] - arr[:,0]\n",
    "    dx = dx.reshape(-1,1)\n",
    "    dx = dx[:-1,:]\n",
    "    \n",
    "    #dv\n",
    "    dv = arr[:,3] - arr[:,1]\n",
    "    dv = dv.reshape(-1,1)\n",
    "    dv = dv[:-1,:]\n",
    "    \n",
    "    #vf and a\n",
    "    vf = arr[:,1]\n",
    "    a = np.diff(vf)*10\n",
    "    vf = vf.reshape(-1,1)\n",
    "    vf = vf[:-1,:]\n",
    "    vf_later = vf[1:,:]\n",
    "    a = a.reshape(-1,1)\n",
    "    a.shape\n",
    "    return np.hstack([dx, dv, vf,a])\n",
    "\n",
    "\n",
    "with open(os.path.join('..', 'data', 'idm_data_no_restriction.pickle'),'rb') as f:\n",
    "    data_pickle = pickle.load(f)\n",
    "\n",
    "xvfl = data_pickle['idm_data'] # x, v of leading and following\n",
    "para = data_pickle['para']\n",
    "\n",
    "# random seed\n",
    "np.random.seed(1234)\n",
    "\n",
    "# test data for final evaluation\n",
    "Eval_num = 10\n",
    "xvfl_test = xvfl[-1*Eval_num:]\n",
    "\n",
    "# state to feature\n",
    "feature_a = list(map(xvfl_to_feature, xvfl[:-1*Eval_num]))\n",
    "feature_a_in_one = np.vstack(feature_a)\n",
    "\n",
    "\n",
    "# get the lb and ub\n",
    "def get_lb_ub(xvfl):\n",
    "    feature_a_in_one_all = np.vstack(list(map(xvfl_to_feature, xvfl)))\n",
    "    lb = [feature_a_in_one_all[:,0].min(), feature_a_in_one_all[:,1].min() ,\n",
    "          feature_a_in_one_all[:,2].min()]\n",
    "    lb = np.array(lb)\n",
    "    ub = [feature_a_in_one_all[:,0].max(), feature_a_in_one_all[:,1].max() ,\n",
    "          feature_a_in_one_all[:,2].max()]\n",
    "    ub = np.array(ub)\n",
    "    return lb, ub\n",
    "lb,ub = get_lb_ub(xvfl)\n",
    "print(lb, ub)\n",
    "\n",
    "def data_split(feature_a_in_one, DataSize, seed = RANDOM_SEED):\n",
    "    feature_a_in_one = feature_a_in_one[:DataSize[\"total\"]]# in case the total number is not consistent\n",
    "    train_val_ext, test = train_test_split(feature_a_in_one, test_size = DataSize['test'], random_state = seed)\n",
    "    train_val, ext = train_test_split(train_val_ext, test_size = DataSize['ext'], random_state = seed)\n",
    "    train, val = train_test_split(train_val, test_size = DataSize['val'], random_state = seed)\n",
    "    X_train = train[:,:3]; a_train = train[:,3].reshape(-1,1)\n",
    "    X_val = val[:,:3]; a_val = val[:,3].reshape(-1,1)\n",
    "    X_test = test[:,:3]; a_test = test[:,3].reshape(-1,1)\n",
    "    \n",
    "    # aux data\n",
    "    X_ext = ext[:, :3]\n",
    "    X_aux = np.concatenate([X_train, X_ext])\n",
    "    return X_train, a_train, X_val, a_val, X_test, a_test, X_aux\n",
    "\n",
    "# Training parameter\n",
    "BATCH_SIZE_X1 = 256\n",
    "BATCH_SIZE_X2 = 30000\n",
    "EPOCH = 100\n",
    "\n",
    "def batch_generator(X, Y, batch_size = BATCH_SIZE_X1):\n",
    "    indices = np.arange(len(X)) \n",
    "    np.random.seed(seed = RANDOM_SEED)\n",
    "    np.random.shuffle(indices) \n",
    "    for i in range(0, len(indices), batch_size):\n",
    "        yield X[i:i+batch_size], Y[i:i+batch_size]\n",
    "        \n",
    "class PhysicsNN():\n",
    "    def __init__(self, alpha, X_train, a_train, X_val, a_val, X_aux, para, layers, lb, ub):\n",
    "        tf.set_random_seed(1234)\n",
    "        self.para = para\n",
    "        self.alpha = alpha\n",
    "        self.X_train = X_train\n",
    "        self.a_train = a_train\n",
    "        self.X_val = X_val\n",
    "        self.a_val = a_val\n",
    "        self.X_aux = X_aux\n",
    "        self.lb = lb\n",
    "        self.ub = ub\n",
    "        self.layers = layers\n",
    "        self.history_loss = []\n",
    "        self.history_loss_val = []\n",
    "        self.history_partial_loss_a = []\n",
    "        self.history_partial_loss_f = []\n",
    "        self.history_diff_a_idm_groundtruth = []\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # input data\n",
    "        # Initialize NNs\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "        self.x_tf = tf.placeholder(tf.float32, shape=[None, self.X_val.shape[1]])#=1, None indicates that the first dimension, corresponding to the batch size, can be of any size\n",
    "        self.a_tf = tf.placeholder(tf.float32, shape=[None, self.a_val.shape[1]])\n",
    "        self.x2_tf = tf.placeholder(tf.float32, shape=[None, self.X_val.shape[1]])\n",
    "\n",
    "        # output and immediate value\n",
    "        # ====== this part is for the PINN==========\n",
    "        #\n",
    "        #\n",
    "        # ==========================================\n",
    "\n",
    "        self.a_pred = self.net_a(self.x_tf)\n",
    "        self.a_idm = self.idm_a(self.x_tf)\n",
    "        self.f_pred = self.net_f(self.x2_tf) # note it should be x2\n",
    "\n",
    "        # loss\n",
    "        #self.loss_a = tf.reduce_mean(tf.square(self.a_tf - self.a_pred))\n",
    "        #self.loss_f = tf.reduce_mean(tf.square(self.f_pred))\n",
    "\n",
    "        self.loss = alpha*tf.reduce_mean(tf.square(self.a_tf - self.a_pred)) + \\\n",
    "                        (1-alpha)*tf.reduce_mean(tf.square(self.f_pred))\n",
    "        #self.diff_a_idm_groundtruth = tf.reduce_mean(tf.square(self.a_idm - self.a_tf))\n",
    "            \n",
    "        \n",
    "        #self.diff_a_idm_groundtruth = self.a_idm - tf.squeeze(self.a_tf)\n",
    "        # optimizer\n",
    "        #self.optimizer = tf.contrib.opt.ScipyOptimizerInterface(self.loss, \n",
    "        #                                                        method = 'L-BFGS-B', \n",
    "        #                                                        options = {'maxiter': 5000000,#Maximum number of iterations\n",
    "        #                                                                   'maxfun': 5000000, #Maximum number of function evaluations\n",
    "        #                                                                   'maxcor': 50, # number of limited memory matric\n",
    "        #                                                                   'maxls': 50, \n",
    "        #                                                                   'ftol' : 1.0 * np.finfo(float).eps})\n",
    "        \n",
    "        self.optimizer_Adam = tf.train.AdamOptimizer()\n",
    "        self.train_op_Adam = self.optimizer_Adam.minimize(self.loss)\n",
    "        \n",
    "        # run session of initilization\n",
    "        init = tf.global_variables_initializer() # after this variables hold the values you told them to hold when you declare them will be made\n",
    "        # tf placeholders and graph\n",
    "        self.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n",
    "                                                     log_device_placement=True))\n",
    "        self.sess.run(init) # initialization run\n",
    "        \n",
    "    def initialize_NN(self, layers):     \n",
    "        tf.set_random_seed(1234)\n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers) \n",
    "        for l in range(0,num_layers-1): # need the -1 because the first and last number in defining the nn is input and output\n",
    "            W = self.xavier_init(size=[layers[l], layers[l+1]])\n",
    "            b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)        \n",
    "        return weights, biases\n",
    "    \n",
    "    def xavier_init(self, size):\n",
    "        tf.set_random_seed(1234)\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]        \n",
    "        xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
    "        return tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
    "    \n",
    "    def neural_net(self, X, weights, biases): # NP\n",
    "        tf.set_random_seed(1234)\n",
    "        \n",
    "        num_layers = len(weights) + 1 # need +1, because # of weights is one dim smaller\n",
    "        \n",
    "        H = 2.0*(X - self.lb)/(self.ub - self.lb) - 1.0  # this is all element-wise operation, centralize and standardize the input data\n",
    "        for l in range(0,num_layers-2):\n",
    "            W = weights[l]\n",
    "            b = biases[l]\n",
    "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "        W = weights[-1]\n",
    "        b = biases[-1]\n",
    "        #Y = tf.exp(tf.add(tf.matmul(H, W), b)) # the final layer has no activation func, so it has to be separetly processed\n",
    "        Y = tf.add(tf.matmul(H, W), b)\n",
    "\n",
    "        return Y\n",
    "    \n",
    "    def net_a(self, X):\n",
    "        tf.set_random_seed(1234)\n",
    "        a = self.neural_net(X, self.weights, self.biases)\n",
    "        return a\n",
    "    \n",
    "    def idm_a(self, X):\n",
    "        tf.set_random_seed(1234)\n",
    "        dx = X[:,0]\n",
    "        dv = X[:,1]\n",
    "        v = X[:,2]\n",
    "        para = self.para #{'v0': 30, 'T': 1.5, 's0': 2, 'a': 0.73, 'b': 1.67}\n",
    "        s0 = para['s0']; v0 = para['v0']; T = para['T']\n",
    "        a = para['a']; b = para['b']\n",
    "        # idm equation\n",
    "        s_star = s0 + T*v - v*dv/2/tf.sqrt(a*b)\n",
    "        acc = a*(1-tf.pow(v/v0, 4) - tf.pow(s_star/dx,2))\n",
    "        \n",
    "        return tf.reshape(acc, (-1,1))\n",
    "    def net_f(self, X):\n",
    "        tf.set_random_seed(1234)\n",
    "        a_nn = self.net_a(X)\n",
    "        a_idm = self.idm_a(X)\n",
    "        f = a_nn - a_idm\n",
    "        return f\n",
    "    \n",
    "    \n",
    "    def callback(self, loss):\n",
    "        #print('Loss: %e, l1: %.5f, l2: %.5f' % (loss, lambda_1, np.exp(lambda_2)))\n",
    "        print('Loss: %e' % (loss))\n",
    "        \n",
    "    def sample_aux(self):\n",
    "        tf.set_random_seed(1234)\n",
    "        #idx = np.random.choice(len(self.X_aux), BATCH_SIZE_X2, replace = False)\n",
    "        idx = np.array([x for x in range(len(self.X_aux))])\n",
    "        return self.X_aux[idx, :]\n",
    "    \n",
    "    def train(self, nIter):\n",
    "        tf.set_random_seed(1234)\n",
    "        start_time = time.time()\n",
    "        #print('start_train')\n",
    "        loss = []\n",
    "        for it in range(nIter):\n",
    "            #train_generator = batch_generator(self.X_train, self.a_train)\n",
    "            #for i, (X, a) in enumerate(train_generator):\n",
    "            #i = i + 1\n",
    "            tf_dict = {self.x_tf: X_train,\n",
    "                       self.a_tf: self.a_train,\n",
    "                      self.x2_tf: self.X_aux }\n",
    "            #if (it == 0):\n",
    "            #    print('==================================================================================')\n",
    "           #        print(\"X shape:{}\".format(X.shape))\n",
    "             #   print(\"aux shape:{}\".format(self.X_aux.shape))\n",
    "           #        print(\"X train shape: {}\".format(slef.X_train.shape))\n",
    "           #        print(\"X val shape: {}\".format(slef.X_val.shape))\n",
    "\n",
    "            self.sess.run(self.train_op_Adam, tf_dict)\n",
    "\n",
    "                \n",
    "             # Print\n",
    "            #if i % 10 == 0: # adam training first.. WHY?\n",
    "            elapsed = time.time() - start_time\n",
    "            loss_value = self.sess.run(self.loss, tf_dict)\n",
    "            #partial_loss_a = self.sess.run(self.loss_a, tf_dict)\n",
    "            #partial_loss_f = self.sess.run(self.loss_f, tf_dict)\n",
    "            #diff_a_idm_groundtruth = self.sess.run(self.diff_a_idm_groundtruth, tf_dict)\n",
    "            \n",
    "            \n",
    "            # val loss\n",
    "            #a_pred_onVal = self.predict(self.X_val)\n",
    "            #loss_val = sum((a_pred_onVal-self.a_val)**2)/len(a_pred_onVal)\n",
    "\n",
    "            #print('It: %d, Loss: %.3e, Lambda_1: %.3f, Lambda_2: %.6f, Time: %.2f' % (it, loss_value, 1.0, 1.0, elapsed))\n",
    "            \n",
    "            #print('Epoch: %d,  Loss: %.3e, Loss_val: %.3e, Time: %.2f' % \n",
    "            #      (it,  loss_value, loss_val,elapsed))\n",
    "            \n",
    "            \n",
    "            #start_time = time.time()\n",
    "            \n",
    "            # cross validation\n",
    "            #if it % Val_freq == 0:\n",
    "            self.history_loss.append(loss_value)\n",
    "            loss.append(loss_value)\n",
    "            #self.history_loss_val.append(loss_val)\n",
    "            #self.history_partial_loss_a.append(partial_loss_a)\n",
    "            #self.history_partial_loss_f.append(partial_loss_f)\n",
    "            #self.history_diff_a_idm_groundtruth.append(diff_a_idm_groundtruth)\n",
    "        #print('Epoch: %d,  Loss: %.3e, Loss_val: %.3e, Time: %.2f' % \n",
    "        #          (it,  loss_value, loss_val,elapsed))\n",
    "        return loss\n",
    "           \n",
    "                \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "    def predict(self, X_star):\n",
    "        \n",
    "        tf_dict = {self.x_tf: X_star}\n",
    "        \n",
    "        a_pred = self.sess.run(self.a_pred, tf_dict)\n",
    "        #f_star = self.sess.run(self.f_pred, tf_dict)\n",
    "        \n",
    "        return a_pred#, f_star\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "    def predict_idm(self, X_star):\n",
    "        tf_dict = {self.x_tf: X_star}\n",
    "        \n",
    "        a_idm = self.sess.run(self.a_idm, tf_dict)\n",
    "        #f_star = self.sess.run(self.f_pred, tf_dict)\n",
    "        \n",
    "        return a_idm#, f_star\n",
    "\n",
    "\n",
    "def simulate(init, XV_L, nn):\n",
    "    xl_0 = init[2]  \n",
    "    vl_0 = init[3]\n",
    "    xf_0 = init[0]\n",
    "    vf_0 = init[1]\n",
    "    assert xl_0 == XV_L[0,0]\n",
    "    assert vl_0 == XV_L[0,1]\n",
    "    XF = [xf_0]\n",
    "    VF = [vf_0]\n",
    "\n",
    "    for i in range(0, len(XV_L)-1):\n",
    "        one_state = np.array([XV_L[i,0], XV_L[i,1], XF[-1], VF[-1]])\n",
    "        feature = np.array([one_state[0]-one_state[2], one_state[1]-one_state[3], one_state[-1]])\n",
    "        a = nn.predict(feature.reshape(-1,3)) # xv_lf\n",
    "        v_next = VF[-1] + 0.1 * a\n",
    "        x_next = (v_next + VF[-1])/2*0.1 + XF[-1]\n",
    "        XF.append(x_next.flatten()[0])\n",
    "        VF.append(v_next.flatten()[0])\n",
    "    return XF, VF\n",
    "\n",
    "# relative error function\n",
    "def get_XV_error(xvfl_test, nn):\n",
    "    XFs = []\n",
    "    VFs = []\n",
    "    XF_tests = []\n",
    "    VF_tests = []\n",
    "    for test_idx in range(len(xvfl_test)):\n",
    "        XF, VF = simulate(xvfl_test[test_idx][0,:], xvfl_test[test_idx][:,2:], nn)\n",
    "        XFs.append(np.array(XF))\n",
    "        VFs.append(np.array(VF))\n",
    "        XF_tests.append(xvfl_test[test_idx][:,0])\n",
    "        VF_tests.append(xvfl_test[test_idx][:,1])\n",
    "\n",
    "    XFs = np.concatenate(XFs)\n",
    "    VFs = np.concatenate(VFs)\n",
    "    XF_tests = np.concatenate(XF_tests)\n",
    "    VF_tests = np.concatenate(VF_tests)\n",
    "\n",
    "    X_error = np.sqrt( sum(np.square(XFs-XF_tests))/sum(np.square(XF_tests)) )\n",
    "    V_error = np.sqrt( sum(np.square(VFs-VF_tests))/sum(np.square(VF_tests)) )\n",
    "    \n",
    "    return X_error, V_error\n",
    "\n",
    "def data_revise_nn(num_train, seed = 1234):\n",
    "    np.random.seed(seed)\n",
    "    DataSize = {\"train\": num_train,\n",
    "               \"ext\": 30000,\n",
    "               \"val\": int(0.4*num_train),\n",
    "               \"test\": 30000}\n",
    "    DataSize[\"total\"] = sum(DataSize.values())\n",
    "    \n",
    "    idx = np.random.choice(len(feature_a_in_one), DataSize[\"total\"] , replace = False) # feature_a_in_one is global\n",
    "    feature_a_in_one_dowsample = feature_a_in_one[idx]\n",
    "    X_train, a_train, X_val, a_val, X_test, a_test, X_aux = data_split(feature_a_in_one_dowsample, DataSize, seed = seed)\n",
    "    \n",
    "    # model\n",
    "    \n",
    "    #nn.X_train = X_train\n",
    "    #nn.a_train = a_train\n",
    "    #nn.X_val = X_val\n",
    "    #nn.a_val = a_val\n",
    "    #nn.X_aux = X_aux\n",
    "    BATCH_SIZE_X2 = len(X_aux)  # global variable\n",
    "    return X_train, a_train, X_val, a_val, X_test, a_test, X_aux\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def nn_test_alpha_beta(nn):\n",
    "    print('X_train shape: {}'.format(nn.X_train.shape))\n",
    "    print('X_val shape: {}'.format(nn.X_val.shape))\n",
    "    \n",
    "    #tf.reset_default_graph()\n",
    "    #nn.loss = alpha * tf.reduce_mean(tf.square(nn.a_tf - nn.a_pred)) + \\\n",
    "    #                  beta * tf.reduce_mean(tf.square(nn.f_pred))\n",
    "    \n",
    "    ## train\n",
    "    #nn.train_op_Adam = nn.optimizer_Adam.minimize(nn.loss)\n",
    "    #init = tf.global_variables_initializer()\n",
    "    #nn.sess.run(init)\n",
    "    nn.train(EPOCH)\n",
    "    \n",
    "    # test_mse\n",
    "    a_test_star = nn.predict(X_test)\n",
    "    #a_test_star.shape\n",
    "    a_test_star = nn.predict(X_test)\n",
    "    test_mse = np.mean(np.square(a_test_star - a_test))\n",
    "    \n",
    "    # X error V error\n",
    "    x_error, v_error = get_XV_error(xvfl_test, nn)\n",
    "    \n",
    "    return test_mse, x_error, v_error\n",
    "\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ga\n",
    "# GA\n",
    "from GA.GA import *\n",
    "\n",
    "args_GA = {\n",
    "    'sol_per_pop': 10,\n",
    "    'num_parents_mating': 5,\n",
    "    'num_mutations': 1, # set 1 to mutate all the parameters\n",
    "    'mutations_extend': 0.1,\n",
    "    'num_generations': 10,\n",
    "    \n",
    "    'delta_t': 0.1,\n",
    "    'mse': 'position',\n",
    "    'RMSPE_alpha_X': 0.5,\n",
    "    'RMSPE_alpha_V': 0.5,\n",
    "    'lb' : [10, 0, 0, 0, 0],\n",
    "    'ub' : [40, 10, 10, 5, 5]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "#del_module(\"GA\")\n",
    "ga = GA(args_GA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [3, 10, 10, 5, 1]\n",
    "Num_trains = [1000]\n",
    "for i in np.arange(5000, 55000, 5000 ):\n",
    "    Num_trains.append(i)\n",
    "Alphas = np.arange(0,1.1,0.1)\n",
    "record = dict()\n",
    "\n",
    "for num_train in Num_trains:\n",
    "    for alpha in Alphas:\n",
    "        Test_mse = []\n",
    "        X_error = []\n",
    "        V_error = []\n",
    "        GA_para = []\n",
    "        for it in range(10):\n",
    "            print(\"num_train: %d, alpha: %.1f  iter: %d\"%(num_train, alpha, it))\n",
    "\n",
    "            tf.reset_default_graph()\n",
    "            start_time = time.time()\n",
    "            X_train, a_train, X_val, a_val, X_test, a_test, X_aux = data_revise_nn(num_train, seed = it)\n",
    "            \n",
    "            #----- from X_train, a_train, get a IDM para\n",
    "            para, mse, duration = ga.executeGA( X_train )\n",
    "            GA_para.append(para)\n",
    "            print('GA duration -> : {}'.format(time.time() - start_time))\n",
    "            \n",
    "            #\n",
    "            nn = PhysicsNN( alpha, X_train, a_train, X_val, a_val, X_aux, para, layers, lb, ub)\n",
    "            test_mse, x_error, v_error = nn_test_alpha_beta(nn)\n",
    "            print('NN duration -> : {}'.format(time.time() - start_time))\n",
    "            Test_mse.append(test_mse)\n",
    "            X_error.append(x_error)\n",
    "            V_error.append(v_error)\n",
    "            break\n",
    "\n",
    "        String = str(num_train) + ',' + str(alpha)\n",
    "        record[String] = dict()\n",
    "        record[String]['Test_mse'] = Test_mse\n",
    "        record[String]['X_error'] = X_error\n",
    "        record[String]['V_error'] = V_error\n",
    "        record[String]['GA_para'] = GA_para\n",
    "        #with open('alpha_curve_avg10_0426_01.pickle', 'wb') as f:\n",
    "        #    pickle.dump(record, f)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
